{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача\n",
    "Считать таблицу из credit_train.csv. Построить модель предсказывающую целевую переменную open_account_flg. Задача является задачей бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib sklearn_pandas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'age', 'marital_status', 'job_position', 'credit_sum',\n",
       "       'credit_month', 'tariff_id', 'score_shk', 'education', 'living_region',\n",
       "       'monthly_income', 'credit_count', 'overdue_credit_count',\n",
       "       'open_account_flg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_orig = pd.read_csv('./credit_train.csv', sep=';', index_col='client_id')\n",
    "\n",
    "df = df_orig.copy(deep=True)\n",
    "target = df['open_account_flg']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Приводим к нужным типам и заполняем пустые значения\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn_pandas import CategoricalImputer\n",
    "import sanitizer\n",
    "\n",
    "df = sanitizer.sanitize_frame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=8, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=2500, multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='roc_auc',\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "\n",
    "X = df.drop(columns=['open_account_flg'])\n",
    "y = df['open_account_flg']\n",
    "\n",
    "cv=8\n",
    "max_iter=2500\n",
    "n_jobs=-1\n",
    "random_state=42\n",
    "\n",
    "clf_rocauc = LogisticRegressionCV(max_iter=max_iter, cv=cv, scoring='roc_auc', n_jobs=n_jobs, random_state=random_state)\n",
    "clf_rocauc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max roc_auc: 0.741466080662243\n"
     ]
    }
   ],
   "source": [
    "print ('Max roc_auc: {}'.format(clf_rocauc.scores_[1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=8, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=2500, multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_f1 = LogisticRegressionCV(max_iter=max_iter, cv=cv, scoring='f1', n_jobs=n_jobs, random_state=random_state)\n",
    "clf_f1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max f1: 0.148832909637162\n"
     ]
    }
   ],
   "source": [
    "print ('Max f1: {}'.format(clf_f1.scores_[1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка\n",
    "Для подготовки модели были произведены следующие действия:\n",
    " - Обработка категориальных признаков(через one hot encoding)\n",
    " - Нормализация некоторых непрерывных признаков(monthly_income и т.д.)\n",
    " - Добавление новых признаков на основе одного или нескольких(credit_history_ratio, dist и т.д.)\n",
    " - Приведение к нормальной форме living_regions. Столбец довольно грязный, поэтому использовался yandex geocoder API для получения \"чистых\" локаций и их координат\n",
    " - Удаление несущественных признаков(gender)\n",
    " \n",
    "#### Обучение\n",
    "После подготовки данных модель обучалась на 8 фолдах используя логистическую регрессию с метриками roc_auc и f1. Принципиально не использовались другие алгоритмы, интересно было посмотреть именно на перфоманс логистической регрессии\n",
    "\n",
    "#### Оценка\n",
    " - Из-за не очень сбалансированных классов решил не использовать метрику accuracy. Так как она показывала не очень стабильные результаты(оно и ясно, в принципе, из описания данной метрики)\n",
    " - Попробовал измерить скор модели на двух метриках - roc_auc и f1. Результаты сверху. По rocauc получаем более менее приемлимые 74 процента. По f1 получились уже скудненькие 14.8%. Мои предположения, что логистическая регрессия для данной задачи не самый оптимальный вариант, поэтому такие вот результаты\n",
    "\n",
    "#### Что можно сделать для увеличения скора:\n",
    " - Классы не очень несбалансированы, попробовать применить алгоритм SMOTE\n",
    " - Есть довольно неплохое решение - добавлять smoothed likelihood для каждого категориального столбца. После этого добавлять такую же переменную для пар столбцов, потом для троек и т.д. и т.д. После применить двойную кросс валидацию(double cross-validation). Он в свое время занял призовое место по данному соревнованию\n",
    " $$ Smoothed Likelihood = \\frac{mean(target) + globalmean+\\alpha}{nrows+\\alpha} $$\n",
    "Где globalmean - среднее значение целевой переменной по всей выборке, alpha - коэффициент регуляризации\n",
    " - Попробовать другие алгоритмы для обучения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
