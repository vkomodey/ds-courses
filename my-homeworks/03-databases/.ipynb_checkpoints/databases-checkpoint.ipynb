{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Задание\n",
    "\n",
    "Взять произвольный текст. Распарсить его на токены(слова). Произвести запись и считывание в и из базы данных. Использовать различные базы данных и различные библиотеки для доступа к ним."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MySQL/MariaDB`, `PostgreSQL`, `Oracle`, `MongoDB `являются в большей степени транзакционными(OLTP) базами данных и не предназначены для хранения большого объема данных. Обычно они используются как hot хранилища, в котороых содержится небольшое количество \"свежих\" данных. Для остальных данных используются так называемые \"cold\" хранилища, которые неплохо справляются с большими данными. Но есть и еще один уровень - так называемые \"warehouses\"(склады) - это то место, которое используется в том случае, когда никакая база данных не может справиться с такими объемами. Обычно время чтения в таких базах данных очень высокая(я имею в виду, что оно может измеряться часами). Яркий пример - https://aws.amazon.com/glacier/ - это натуральные магнитные ленты, которые лежат в шкафах. И когда пользователь запрашивает соотвествующую информацию(например, историю своих заказов 15 лет назад), выезжает специальный робот, берет эту ленту и производит чтение с ленты и запись на жесткий диск.\n",
    "\n",
    "Понятное дело, что я не буду подымать кластер  `Amazon Glacier`, так как нам для наших целей это не очень подходит. Поэтому в данном задании будет развернуто одно `OLTP` решение и одно `OLAP` решение. Свой выбор объясняю тем, что в небольших компаниях аналитика может проводиться на основной базе - а она как раз обычно `OLTP`. Когда проект/компания подрастает, то они начинают масштабироваться и уже испольовать `OLAP `для анализа.\n",
    "\n",
    "Примеры OLAP решений - `Cassandra`, `Druid`, `Clickhouse`, `Hadoop + Apache Spark`.\n",
    "\n",
    "OLTP BD - `PostgreSQL`. Стандарт `OLTP `решений во многих компаниях. `PostgreSQL `импонирует мне тем, что данная база имеет довольно богатый query language + включает в себя поддержку фильтрации по JSON документам. В прошлые годы `Postgres` обходил `MongoDB` по скорости работы с JSON документами\n",
    "OLAP BD - `ClickHouse`. Довольно амбициозный проект от компании Yandex. Представляет собой колоночную аналитическую базу данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачивание и настройка PostgreSQL\n",
    "Для простоты был использован `Docker` образ\n",
    "\n",
    "```\n",
    "docker run --name ds-postgres -d -e POSTGRES_PASSWORD=\"asd123\" -e POSTGRES_USER=\"admin\" -e POSTGRES_DB=\"text\" -p 5432:5432 postgres:12-alpine\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание и парсинг книги \"Гарри Поттер и методы рационального мышления\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FILENAME='./pom.txt'\n",
    "\n",
    "with open(FILENAME) as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[\\n\\r\\s\\.,\\'\\\"\\(\\):\\!\\?\\+\\d]+', data)\n",
    "\n",
    "tokens = list(filter(lambda x: len(x) > 4, tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с PostgreSQL в Python скриптах использовался psycopg2 как самый популярный драйвер на данный момент. Список доступных драйверов:\n",
    "* `Psycopg2`\n",
    "* `pg8000`\n",
    "* `py-postgresql`\n",
    "* `PyGreSQL`\n",
    "* `ocpgdb`\n",
    "* `bpgsql`\n",
    "* `SQLAlchemy`\n",
    "\n",
    "```\n",
    "pip install psycopg2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "config_object = {\n",
    "    'dbname': 'text',\n",
    "    'user': 'admin', \n",
    "    'password': 'asd123',\n",
    "    'host': '0.0.0.0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "connection = psycopg2.connect(**config_object)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('CREATE TABLE IF NOT EXISTS tokens (id serial PRIMARY KEY, token varchar)')\n",
    "cursor.execute('DELETE FROM tokens')\n",
    "cursor.execute('INSERT INTO tokens (token) VALUES ' + ('(%s), '*len(tokens))[: -2], tokens)\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Psycopg2 write time: {}s. Rows amount = {}\".format(end - start, len(tokens)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "connection = psycopg2.connect(**config_object)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('SELECT token FROM tokens')\n",
    "\n",
    "data = cursor.fetchall()\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Psycopg2 read time: {}s. Rows amount = {}\".format(end - start, len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проведем те же манипуляции с другим драйвером - `sqlalchemy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlalchemy write time: 1.6546473503112793s. Rows amount = 317063\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as db\n",
    "config_object = {\n",
    "    'database': 'text',\n",
    "    'password': 'asd123',\n",
    "    'host': '0.0.0.0'\n",
    "}\n",
    "engine = db.create_engine('postgresql://admin:asd123@0.0.0.0:5432/text')\n",
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "connection.execute('CREATE TABLE IF NOT EXISTS tokens (id serial PRIMARY KEY, token varchar)')\n",
    "connection.execute('DELETE FROM tokens')\n",
    "connection.execute('INSERT INTO tokens (token) VALUES ' + (\"('{}'), \"*len(tokens))[: -2].format(*tokens))\n",
    "\n",
    "connection.close()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"sqlalchemy write time: {}s. Rows amount = {}\".format(end - start, len(tokens)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlalchemy read time: 0.34694480895996094s. Rows amount = 317063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "connection = engine.connect()\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fetchQuery = connection.execute('SELECT * FROM tokens')\n",
    "\n",
    "fetchQuery.fetchall()\n",
    "\n",
    "connection.close()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"sqlalchemy read time: {}s. Rows amount = {}\".format(end - start, len(tokens)),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачивание и настройка Clickhouse. \n",
    "\n",
    "Как обычно, скачиваем и запускаем соответствующий `docker` образ\n",
    "\n",
    "```\n",
    "docker run -d --name ds-clickhouse-server --ulimit nofile=262144:262144 -d -p 8123:8123 -p 9000:9000 yandex/clickhouse-server\n",
    "```\n",
    "\n",
    "Для запуска есть соответствующий контейнер\n",
    "\n",
    "```\n",
    "docker run -it --rm --link ds-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для доступа к базе данных установим `clickhouse-driver` из `pip` репозитория:\n",
    "```\n",
    "pip install clickhouse-driver\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickhouse driver write time: 0.7034258842468262s. Rows amount = 317063\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "client = Client(host='0.0.0.0')\n",
    "\n",
    "client.execute('CREATE DATABASE IF NOT EXISTS text;')\n",
    "client.execute('DROP TABLE IF EXISTS text.tokens;');\n",
    "client.execute('CREATE TABLE IF NOT EXISTS text.tokens (token String) Engine = Log;')\n",
    "client.execute('INSERT INTO text.tokens (token) VALUES', list(map(lambda x: [x], tokens)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Clickhouse driver write time: {}s. Rows amount = {}\".format(end - start, len(tokens)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickhouse driver read time: 0.4524197578430176s. Rows amount = 317063\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "values = client.execute('SELECT * FROM text.tokens');\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Clickhouse driver read time: {}s. Rows amount = {}\".format(end - start, len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат работы с примерно 300 000 строками\n",
    "\n",
    "| Operation     | Clickhouse      | PostgreSQL(psycopg2)  | PostgreSQL(SQLAlchemy)  |\n",
    "| ------------- |:---------------:| ---------------------:| -----------------------:|\n",
    "| read          | 0.45s           | 0.18s                 | 0.3s                    |\n",
    "| write         | 1.65s           | 0.65s                 | 1.8s                    |\n",
    "\n",
    "Мы записали в базу сразу довольно много значений. Как и ожидалось OLTP решение довольно долго это делало, в то время как\n",
    "ClickHouse изначально заточенный под такие операции справился почти в 3 раза быстрее. Но, в то же время, PostgreSQL выиграл\n",
    "в скорости чтения. Тоже довольно логично. Итог - для каждой задачи свой инструмент."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
